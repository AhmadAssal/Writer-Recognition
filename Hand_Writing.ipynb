{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2 as cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, factor=1,name=\"image\"):\n",
    "    \"\"\" \n",
    "    show an image until the escape key is pressed\n",
    "    :param factor: scale factor (default 1, half size)\n",
    "    \"\"\"\n",
    "    if factor != 1.0:\n",
    "        img = cv2.resize(img, (0,0), fx=factor, fy=factor) \n",
    "\n",
    "    cv2.imshow(name,img)\n",
    "    while(1):\n",
    "        k = cv2.waitKey(0)\n",
    "        if k==27:    # Esc key to stop\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(input_path):\n",
    "    img = cv2.imread(input_path,0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel(img, center, x, y): \n",
    "      \n",
    "    new_value = 0\n",
    "      \n",
    "    try: \n",
    "        # If local neighbourhood pixel  \n",
    "        # value is greater than or equal \n",
    "        # to center pixel values then  \n",
    "        # set it to 1 \n",
    "        if img[x][y] >= center: \n",
    "            new_value = 1\n",
    "              \n",
    "    except: \n",
    "        # Exception is required when  \n",
    "        # neighbourhood value of a center \n",
    "        # pixel value is null i.e. values \n",
    "        # present at boundaries. \n",
    "        pass\n",
    "      \n",
    "    return new_value \n",
    "   \n",
    "# Function for calculating LBP \n",
    "def lbp_calculated_pixel(img, x, y): \n",
    "   \n",
    "    center = img[x][y] \n",
    "   \n",
    "    val_ar = [] \n",
    "      \n",
    "    # top_left \n",
    "    val_ar.append(get_pixel(img, center, x-1, y-1)) \n",
    "      \n",
    "    # top \n",
    "    val_ar.append(get_pixel(img, center, x-1, y)) \n",
    "      \n",
    "    # top_right \n",
    "    val_ar.append(get_pixel(img, center, x-1, y + 1)) \n",
    "      \n",
    "    # right \n",
    "    val_ar.append(get_pixel(img, center, x, y + 1)) \n",
    "      \n",
    "    # bottom_right \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y + 1)) \n",
    "      \n",
    "    # bottom \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y)) \n",
    "      \n",
    "    # bottom_left \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y-1)) \n",
    "      \n",
    "    # left \n",
    "    val_ar.append(get_pixel(img, center, x, y-1)) \n",
    "       \n",
    "    # Now, we need to convert binary \n",
    "    # values to decimal \n",
    "    power_val = [1, 2, 4, 8, 16, 32, 64, 128] \n",
    "   \n",
    "    val = 0\n",
    "      \n",
    "    for i in range(len(val_ar)): \n",
    "        val += val_ar[i] * power_val[i] \n",
    "          \n",
    "    return val \n",
    "   \n",
    "\n",
    "def get_LBP(sentence,show_steps=1,show_size=0.2):\n",
    "\n",
    "    height, width = sentence.shape \n",
    "\n",
    "\n",
    "    # Create a numpy array as  \n",
    "    # the same height and width  \n",
    "    # of RGB image \n",
    "    img_lbp = np.zeros((height, width), np.uint8) \n",
    "\n",
    "    for i in range(0, height): \n",
    "        for j in range(0, width): \n",
    "            img_lbp[i, j] = lbp_calculated_pixel(sentence, i, j) \n",
    "    return img_lbp\n",
    "#img = get_LBP(sentences[0])\n",
    "#show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img,show_steps=1,show_size=0.2):\n",
    "    img = cv2.GaussianBlur(img,(11,11),0)\n",
    "    thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    img[img >= thresh[0]] = 255\n",
    "    img[img <= thresh[0]] = 0\n",
    "    img = cv2.bitwise_not(img)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(2,2))\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "    if show_steps == 1:        \n",
    "        show(img,show_size,\"Preprocessed Image\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_top(img,show_steps=1,show_size=0.2):\n",
    "    trsh = 0.1\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    erosion = cv2.dilate(img,kernel,iterations = 8)\n",
    "    proj = np.sum(erosion,axis=1).astype(int)\n",
    "    if show_steps == 1:        \n",
    "        show(erosion,show_size,\"Eroded\")\n",
    "    max_line = np.amax(proj)\n",
    "\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    lines.append(300)\n",
    "    for i in range(img.shape[0]):\n",
    "        if(proj[i] >= trsh*max_line) :\n",
    "            if(len(lines) > 0 ):\n",
    "                if(lines[-1] + img.shape[0]/10 > i ):\n",
    "                    continue\n",
    "            if( (i>=500 and i <= 900) or (i>=2600 and i <= 2900)):\n",
    "                lines.append(i)\n",
    "    if show_steps == 1:        \n",
    "        show(img[lines[1]:lines[2],:],show_size,\"Cropped Image\")\n",
    "    \n",
    "    return img[lines[1]:lines[2],:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(img,show_steps=1,show_size=0.2):\n",
    "    \n",
    "    original = img.copy()\n",
    "    blurred = cv2.GaussianBlur(img, (1, 1), 0)\n",
    "    canny = cv2.Canny(blurred, 50, 255, 1)\n",
    "    kernel = np.ones((1,5),np.uint8)\n",
    "    dilate = cv2.dilate(canny, kernel, iterations=40)\n",
    "    \n",
    "    if show_steps == 1:        \n",
    "        show(dilate,show_size,\"Sentences\")\n",
    "    \n",
    "    # Find contours\n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    # Iterate thorugh contours and filter for ROI\n",
    "    image_number = 0\n",
    "    order_list = []\n",
    "    images = []\n",
    "    area = 0 \n",
    "    for c in cnts:\n",
    "        area += cv2.contourArea(c)\n",
    "    area /= (len(cnts)+5)\n",
    "    for c in cnts:\n",
    "        if(cv2.contourArea(c) > area):\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            if h > img.shape[0]/8:\n",
    "                continue\n",
    "            #cv2.rectangle(img, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "            ROI = original[y:y+h, x:x+w]\n",
    "            if show_steps == 1:        \n",
    "                show(ROI,show_size,\"Sentence\")\n",
    "            images.append(ROI)\n",
    "            order_list.append(y+x/1000)\n",
    "            image_number += 1\n",
    "    segments = [x for _,x in sorted(zip(order_list,images))]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img):\n",
    "    mask_inv = img\n",
    "    ver_sum = np.sum(mask_inv,axis=1)\n",
    "    v_start = 0\n",
    "    v_end = 0\n",
    "    for i in range(len(ver_sum)):\n",
    "        if(ver_sum[i] > 0 and v_start ==0):\n",
    "            v_start = i\n",
    "        if(ver_sum[i] == 0 and v_start != 0):\n",
    "            v_end = i\n",
    "            break\n",
    "    if(v_end == 0):\n",
    "        v_end = len(ver_sum) - 1\n",
    "    \n",
    "    hor_sum = np.sum(mask_inv,axis=0)\n",
    "    h_start = 0\n",
    "    h_end = 0\n",
    "    for i in range(len(hor_sum)):\n",
    "        if(hor_sum[i] > 0 and h_start ==0):\n",
    "            h_start = i\n",
    "        if(hor_sum[i] == 0 and h_start != 0):\n",
    "            h_end = i\n",
    "            break\n",
    "    if(h_end == 0):\n",
    "        h_end = len(hor_sum) - 1\n",
    "\n",
    "    return img[v_start:v_end,h_start:h_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_horizontal_merge(sentence,show_steps=1,show_size=0.2):\n",
    "    original = np.copy(sentence)\n",
    "    \n",
    "    # Find contours\n",
    "    cnts = cv2.findContours(sentence, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    # Iterate thorugh contours and filter for ROI\n",
    "    order_list = []\n",
    "    images = []\n",
    "    area = 0 \n",
    "    for c in cnts:\n",
    "        area += cv2.contourArea(c)\n",
    "    area /= len(cnts)\n",
    "    for c in cnts:\n",
    "        if(cv2.contourArea(c) > area/10):\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            ROI = original[:,x:x+w]\n",
    "            images.append(ROI)\n",
    "            order_list.append(x)\n",
    "\n",
    "    hori_merged = np.zeros((original.shape[0],original.shape[1]))\n",
    "    segments = [sorter for _,sorter in sorted(zip(order_list,images))]\n",
    "    current = 0\n",
    "    for segment in segments:\n",
    "        hori_merged[:,current:current+segment.shape[1]] = segment\n",
    "        current = current+segment.shape[1]\n",
    "    hori_merged = crop_img(hori_merged)\n",
    "    return hori_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_image(image):\n",
    "    copy = np.zeros((image.shape[0], image.shape[1]))\n",
    "    #print(\"Image shape is \", image.shape, \", copy shape is \", copy.shape)\n",
    "    sentences = get_sentences(image,0)\n",
    "    contours, hierarchy = cv2.findContours(image.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    avg_height = 0\n",
    "    l = len(contours)\n",
    "    for contour in contours:\n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        avg_height += h\n",
    "    avg_height = int(avg_height/l)\n",
    "    print(\"avg h\",avg_height)\n",
    "    currentY = 0\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        #print(currentY)\n",
    "        sentence = get_horizontal_merge(sentence,0)\n",
    "        copy[currentY:currentY+sentence.shape[0],0:sentence.shape[1]] += sentence\n",
    "        #print(\"Copy after \", i, \" sentence(s)\")\n",
    "        #show(copy, 0.2, \"copy\")\n",
    "        currentY += int(avg_height/2)\n",
    "        \n",
    "    return copy[:currentY+ int(avg_height),:]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg h 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-aedc9f752255>:22: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  segments = [sorter for _,sorter in sorted(zip(order_list,images))]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (117,30) (117,151) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9b106de4e9c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# cropped = cropImage(dilation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# show(cropped, 0.2, \"Cropped\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mrearranged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrearrange_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrearranged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Rearranged\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-eb7cb75116a2>\u001b[0m in \u001b[0;36mrearrange_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#print(currentY)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_horizontal_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrentY\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print(\"Copy after \", i, \" sentence(s)\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-aedc9f752255>\u001b[0m in \u001b[0;36mget_horizontal_merge\u001b[1;34m(sentence, show_steps, show_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mhori_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0msegments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msorter\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msorter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msegment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msegments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (117,30) (117,151) "
     ]
    }
   ],
   "source": [
    "input_path = 'F:/Tech/CUFE_CHS/Fall_2020/Pattern/Project/Data_Set/a01-003.png'\n",
    "show_steps = 1\n",
    "show_size = 0.2\n",
    "\n",
    "images = []\n",
    "#for file in os.listdir(input_path):\n",
    "#    images.append(file)\n",
    "#images.sort()\n",
    "#for input_img in images:   \n",
    "#show(im)\n",
    "img = read_image(input_path)#+input_img)\n",
    "img = preprocess_img(img,show_steps,show_size)\n",
    "img = remove_top(img,show_steps,show_size)\n",
    "\n",
    "# kernel = np.ones((5,5))\n",
    "# dilation = cv2.dilate(img,kernel,iterations = 20)\n",
    "sentences = get_sentences(img,show_steps,show_size)\n",
    "# show(dilation, 0.2, \"Dilated\")\n",
    "# cropped = cropImage(dilation)\n",
    "# show(cropped, 0.2, \"Cropped\")\n",
    "rearranged = rearrange_image(img)\n",
    "show(rearranged, 2, \"Rearranged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for testing the read data code\n",
    "for i in images:\n",
    "    for img in i:\n",
    "        show(img, 0.5)\n",
    "    print(\"finished folder\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(folder_name):\n",
    "    input_path = \"/media/madmax/MadMax_D/CUFE/Fall_2021/CMPN450_PatternRecognition_and_NeuralNetworks/Project document/Writer-Recognition/data/\" + folder_name + \"/\"\n",
    "    images = []\n",
    "    labeles=[]\n",
    "    # iterate over all 3 writers and add their feature vectors\n",
    "    for i in range(1, 4):\n",
    "        newpath = input_path + str(i) + \"/\"\n",
    "        for file in os.listdir(newpath):\n",
    "            images.append(cv2.imread(newpath + file))\n",
    "            labeles.append(i)\n",
    "#     print(len(images))\n",
    "#     print(len(labeles))\n",
    "\n",
    "    return images, labeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show(original, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases_count=10\n",
    "images = []\n",
    "lables=[]\n",
    "for test_cases in range (1,test_cases_count + 1):\n",
    "    folder_name = str(test_cases)\n",
    "    while len(folder_name)<2:\n",
    "        folder_name = \"0\" + folder_name\n",
    "    images_temp,lables_temp = training_data(folder_name)\n",
    "    images.append(images_temp)\n",
    "    lables.append(lables_temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
